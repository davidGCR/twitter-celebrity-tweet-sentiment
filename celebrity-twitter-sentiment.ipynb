{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from kinnara.gather.twitter import TwitterApiWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read in api keys and access tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('API_KEY')\n",
    "api_secret = os.getenv('API_SECRET')\n",
    "\n",
    "access_token = os.getenv('ACCESS_TOKEN')\n",
    "access_token_secret = os.getenv('ACCESS_TOKEN_SECRET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather public figure tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create kinnara gatherer\n",
    "twitter_wrapper = TwitterApiWrapper(api_key=api_key, api_secret=api_secret,\n",
    "                        access_token=access_token, access_token_secret=access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_names = [\n",
    "    'BarackObama',\n",
    "    'realDonaldTrump',\n",
    "    'KimKardashian',\n",
    "    'BillGates',\n",
    "    'Oprah',\n",
    "    'justinbieber',\n",
    "    'TheRock',\n",
    "    'elonmusk',\n",
    "    'JeffBezos',\n",
    "    'katyperry'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-25 18:02:23,745 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:23,927 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:24,103 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:24,213 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:24,359 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:24,502 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:24,645 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:24,829 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:24,960 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:25,137 - INFO - Starting new HTTPS connection (1): api.twitter.com\n"
     ]
    }
   ],
   "source": [
    "users = []\n",
    "for screen_name in screen_names:\n",
    "    users.append(twitter_wrapper.get_user(screen_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-25 18:02:36,392 - INFO - getting tweets for 813286\n",
      "2018-08-25 18:02:36,397 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:36,801 - INFO - getting tweets for 813286\n",
      "2018-08-25 18:02:36,804 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:37,186 - INFO - getting tweets for 25073877\n",
      "2018-08-25 18:02:37,189 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:37,566 - INFO - getting tweets for 25073877\n",
      "2018-08-25 18:02:37,569 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:37,988 - INFO - getting tweets for 25365536\n",
      "2018-08-25 18:02:37,991 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:38,488 - INFO - getting tweets for 25365536\n",
      "2018-08-25 18:02:38,491 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:39,123 - INFO - getting tweets for 50393960\n",
      "2018-08-25 18:02:39,126 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:39,573 - INFO - getting tweets for 50393960\n",
      "2018-08-25 18:02:39,575 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:40,256 - INFO - getting tweets for 19397785\n",
      "2018-08-25 18:02:40,259 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:40,867 - INFO - getting tweets for 19397785\n",
      "2018-08-25 18:02:40,869 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:41,235 - INFO - getting tweets for 27260086\n",
      "2018-08-25 18:02:41,239 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:41,779 - INFO - getting tweets for 27260086\n",
      "2018-08-25 18:02:41,782 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:42,175 - INFO - getting tweets for 250831586\n",
      "2018-08-25 18:02:42,178 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:42,596 - INFO - getting tweets for 250831586\n",
      "2018-08-25 18:02:42,599 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:43,156 - INFO - getting tweets for 44196397\n",
      "2018-08-25 18:02:43,159 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:43,519 - INFO - getting tweets for 44196397\n",
      "2018-08-25 18:02:43,522 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:43,786 - INFO - getting tweets for 15506669\n",
      "2018-08-25 18:02:43,788 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:44,134 - INFO - getting tweets for 21447363\n",
      "2018-08-25 18:02:44,136 - INFO - Starting new HTTPS connection (1): api.twitter.com\n",
      "2018-08-25 18:02:44,688 - INFO - getting tweets for 21447363\n",
      "2018-08-25 18:02:44,691 - INFO - Starting new HTTPS connection (1): api.twitter.com\n"
     ]
    }
   ],
   "source": [
    "# lets grab 400 tweets from each twitter users timeline\n",
    "for user in users:\n",
    "    user['tweets'] = twitter_wrapper.get_tweets(user['id_str'], max_tweets_returned=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet_text):\n",
    "    '''clean up tweet text'''\n",
    "    return re.sub(r' ?https[^ ]*', r'', tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_name_to_tweets = {}\n",
    "for user in users:\n",
    "    screen_name_to_tweets[user['screen_name']] = [clean_tweet(tweet.get('full_text', ''))\n",
    "                                                  for tweet in user['tweets']\n",
    "                                                  if re.match(r'^RT @', tweet['full_text']) is None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_names = []\n",
    "tweets = []\n",
    "\n",
    "for k, ts in screen_name_to_tweets.items():\n",
    "    for tweet in ts:\n",
    "        screen_names.append(k)\n",
    "        tweets.append(tweet)\n",
    "\n",
    "celebrity_df = pd.DataFrame.from_dict({'screen_names': screen_names, 'tweets': tweets})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it for later\n",
    "celebrity_df.to_csv('/home/ubuntu/data/twitter/celebrity_tweets.csv', header=False, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This portion of the notebook borrows extensively from fast.ai's notebook for lesson 10 in their deep learning course part two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import html\n",
    "\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the directories we'll be using\n",
    "\n",
    "You'll need to download tweet sentiment data from kaggle - https://www.kaggle.com/c/twitter-sentiment-analysis2/data - and put it in the twitter/orignal directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos'  # beginning-of-sentence tag\n",
    "FLD = 'xfld'  # data field tag\n",
    "\n",
    "# path to our original tweet data from kaggle\n",
    "PATH = '/home/ubuntu/data/twitter/original'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to our classification data\n",
    "CLASSIFICATION_PATH = '/home/ubuntu/data/twitter/twitter_classification/'\n",
    "\n",
    "# path to our language model\n",
    "LANGUAGE_MODEL_PATH = '/home/ubuntu/data/twitter/twitter_language_model/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start prepping our tweet data from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['neg', 'pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH + '/train.csv', encoding='latin1')\n",
    "\n",
    "# shuffle rows and throw out item id column\n",
    "df = df.drop('ItemID', axis=1)\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# rename columns\n",
    "df.columns = ['labels', 'text']\n",
    "\n",
    "# split datafram into train and validation sets for later\n",
    "split_index = int(df.shape[0] * .9)\n",
    "train_df, test_df = np.split(df, [split_index], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving our current progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to classification directory\n",
    "train_df.to_csv(CLASSIFICATION_PATH + '/train.csv', header=False, index=False, encoding='utf-8')\n",
    "test_df.to_csv(CLASSIFICATION_PATH + '/test.csv', header=False, index=False, encoding='utf-8')\n",
    "\n",
    "f = open(CLASSIFICATION_PATH + '/classes.txt', 'w', encoding='utf-8')\n",
    "f.writelines(f'{c}\\n' for c in CLASSES)\n",
    "f.close()\n",
    "\n",
    "# save to language model directory\n",
    "# we should be adding in the test.csv from the kaggle competition here because the language model doesn't care about labels\n",
    "# but in the interest of time I'm opting to just use the training set for the language model\n",
    "train_df.to_csv(LANGUAGE_MODEL_PATH + '/train.csv', header=False, index=False, encoding='utf-8')\n",
    "test_df.to_csv(LANGUAGE_MODEL_PATH + '/test.csv', header=False, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets start cleaning some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunksize for pandas so it doesn't run into any memory limits\n",
    "chunksize=24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions pulled from the fast.ai notebook for text tokenization\n",
    "\n",
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    '''some patterns identified by the fast.ai folks that spacy doesn't account for'''\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "def get_texts(df, n_lbls=1):\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "\n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok, list(labels)\n",
    "\n",
    "def get_all(df, n_lbls):\n",
    "    '''tokenize the text'''\n",
    "    tok, labels = [], []\n",
    "    for i, r in enumerate(df):\n",
    "        tok_, labels_ = get_texts(r, n_lbls)\n",
    "        tok += tok_;\n",
    "        labels += labels_\n",
    "    return tok, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab our dataframes from earlier\n",
    "train_df = pd.read_csv(LANGUAGE_MODEL_PATH + '/train.csv', header=None, chunksize=chunksize)\n",
    "val_df = pd.read_csv(LANGUAGE_MODEL_PATH + '/test.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the tweets\n",
    "train_tokens, train_labels = get_all(train_df, 1)\n",
    "val_tokens, val_labels = get_all(val_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make temporary directory\n",
    "os.mkdir(LANGUAGE_MODEL_PATH + '/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our tokens\n",
    "np.save(LANGUAGE_MODEL_PATH + '/tmp/tok_trn.npy', train_tokens)\n",
    "np.save(LANGUAGE_MODEL_PATH + '/tmp/tok_val.npy', val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back in\n",
    "train_tokens = np.load(LANGUAGE_MODEL_PATH + '/tmp/tok_trn.npy')\n",
    "val_tokens = np.load(LANGUAGE_MODEL_PATH + '/tmp/tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take a look at our most common tokens\n",
    "freq = Counter(token for tokens in train_tokens for token in tokens)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks about right.  quick note to keep in mind - the \"t_up\" token isn't in the text its self, it is a marker indicating the following token is all uppercase.\n",
    "\n",
    "fast.ai recommends that you only keep the 60,000 or so most common tokens. Reason being low frequency tokens don't help you learn a lot about a language.\n",
    "\n",
    "We don't have that many tokens for our tweets, but it makes me feel good to put in anyways ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 2\n",
    "\n",
    "int_to_token = [o for o, c in freq.most_common(max_vocab) if c > min_freq]\n",
    "int_to_token.insert(0, '_pad_')\n",
    "int_to_token.insert(0, '_unk_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_int = collections.defaultdict(lambda: 0, {v: k for k, v in enumerate(int_to_token)})\n",
    "len(int_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lm = np.array([[token_to_int[o] for o in p] for p in train_tokens])\n",
    "val_lm = np.array([[token_to_int[o] for o in p] for p in val_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving our progress\n",
    "np.save(LANGUAGE_MODEL_PATH + '/tmp/trn_ids.npy', train_lm)\n",
    "np.save(LANGUAGE_MODEL_PATH + '/tmp/val_ids.npy', val_lm)\n",
    "\n",
    "pickle.dump(int_to_token, open(LANGUAGE_MODEL_PATH + '/tmp/itos.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading back in\n",
    "train_lm = np.load(LANGUAGE_MODEL_PATH + '/tmp/trn_ids.npy')\n",
    "val_lm = np.load(LANGUAGE_MODEL_PATH + '/tmp/val_ids.npy')\n",
    "int_to_token = pickle.load(open(LANGUAGE_MODEL_PATH + '/tmp/itos.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_twitter_tokens = len(int_to_token)\n",
    "num_twitter_tokens, len(train_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in a pretrained language model trained on wikipedia text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run this line to download wikipedia model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -nH -r -np -P {PATH} http://files.fast.ai/models/wt103/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some stats from the wikepedia model\n",
    "embedding_size, num_hidden, num_layers = 400,1150,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_PATH = '/home/ubuntu/data/twitter/original/models/wt103/'\n",
    "PRE_LM_PATH = PRE_PATH + '/fwd_wt103.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the weights from the encoder\n",
    "weights = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the weights from layer 0 can be used to assign weights to tokens that exist in the wikipedia dataset but not in the twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_weights = to_np(weights['0.encoder.weight'])\n",
    "# row_m = enc_wgts.mean(0)\n",
    "encoder_mean = encoder_weights.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_int_to_token = pickle.load(open(PRE_PATH + '/itos_wt103.pkl', 'rb'))\n",
    "wiki_token_to_int = collections.defaultdict(lambda: -1, {v:k for k, v in enumerate(wiki_int_to_token)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to assign mean weights to tokens that exist in our twitter dataset that dont in the wikipedia dataset the pretrained model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = np.zeros((num_twitter_tokens, embedding_size), dtype=np.float32)\n",
    "for i, w in enumerate(int_to_token):\n",
    "    r = wiki_token_to_int[w]\n",
    "    new_weights[i] = encoder_weights[r] if r >= 0 else encoder_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to put the new weights into the pretrained model\n",
    "\n",
    "The weights between the encoder and decoder also need to be tied together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights['0.encoder.weight'] = T(new_weights)\n",
    "weights['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_weights))\n",
    "weights['1.decoder.weight'] = T(np.copy(new_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining the wikipedia language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=1e-7 # weight decay\n",
    "bptt=70 # ngram size.  i.e. the model sees ~70 tokens and then tries to predict the 71st\n",
    "bs=52 # batch size\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99)) # optimazation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a special fastai data loader, the `LanguageModelLoader`, to feed the training data into the model whilst training.\n",
    "\n",
    "We can then use those to instanciate a `LanguageModelData` class that returns a fastai model we can train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = LanguageModelLoader(np.concatenate(train_lm), bs, bptt)\n",
    "val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\n",
    "\n",
    "md = LanguageModelData(PATH, 1, vs, train_dl, val_dl, bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dropouts for each layer.\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last embedding layer needs to be tuned first so the new weights we set for the pretrained model get tuned properly.\n",
    "\n",
    "fastai allows you to freeze and unfreeze model layers.  So here we freeze everything but the weights in the last embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = md.get_model(\n",
    "    opt_fn, embedding_size, num_hidden, num_layers, dropouti=drops[0], dropout=drops[1],\n",
    "    wdrop=drops[2], dropoute=drops[3], dropouth=drops[4]\n",
    ")\n",
    "\n",
    "learner.metrics = [accuracy]\n",
    "\n",
    "# freeze everything except last layer\n",
    "learner.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights\n",
    "learner.model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3 # learning rate\n",
    "lrs = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our progress\n",
    "learner.save('lm_last_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back in\n",
    "learner.load('lm_last_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with our new embedding weights trained up, we can unfreeze and train all layers\n",
    "learner.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find our learning rate\n",
    "learner.lr_find(start_lr=lrs/10, end_lr=lrs*50, linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like 10-2 or 10-3 or so could be a good learning rate for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our progress\n",
    "learner.save('lm1')\n",
    "learner.save_encoder('lm1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at our loss\n",
    "learner.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Sentiment Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our language model trained on tweets, we can start training our tweet sentiment classifier\n",
    "\n",
    "To do this all we have to do is tack on a layer to our trained encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(CLASSIFICATION_PATH + '/train.csv', header=None, chunksize=chunksize)\n",
    "val_df = pd.read_csv(CLASSIFICATION_PATH + '/test.csv', header=None, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same cleaning we did for the language model\n",
    "train_tokens, train_labels = get_all(train_df, 1)\n",
    "val_tokens, val_labels = get_all(val_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make temp directory in classifier directory\n",
    "# os.mkdir(CLASSIFICATION_PATH + '/tmp')\n",
    "\n",
    "# save tokens\n",
    "np.save(CLASSIFICATION_PATH + '/tmp/tok_trn.npy', train_tokens)\n",
    "np.save(CLASSIFICATION_PATH + '/tmp/tok_val.npy', val_tokens)\n",
    "\n",
    "np.save(CLASSIFICATION_PATH + '/tmp/trn_labels.npy', train_labels)\n",
    "np.save(CLASSIFICATION_PATH + '/tmp/val_labels.npy', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back in \n",
    "train_tokens = np.load(CLASSIFICATION_PATH + '/tmp/tok_trn.npy')\n",
    "val_tokens = np.load(CLASSIFICATION_PATH + '/tmp/tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15833"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_token = pickle.load(open(LANGUAGE_MODEL_PATH + '/tmp/itos.pkl', 'rb'))\n",
    "token_to_int = collections.defaultdict(lambda: 0, {v:k for k, v in enumerate(int_to_token)})\n",
    "len(int_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classification = np.array([[token_to_int[o] for o in p] for p in train_tokens])\n",
    "val_classification = np.array([[token_to_int[o] for o in p] for p in val_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(CLASSIFICATION_PATH + '/tmp/trn_ids.npy', train_classification)\n",
    "np.save(CLASSIFICATION_PATH + '/tmp/val_ids.npy', val_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back in \n",
    "train_classification = np.load(CLASSIFICATION_PATH + '/tmp/trn_ids.npy')\n",
    "val_classification = np.load(CLASSIFICATION_PATH + '/tmp/val_ids.npy')\n",
    "\n",
    "train_labels = np.squeeze(np.load(CLASSIFICATION_PATH + '/tmp/trn_labels.npy'))\n",
    "val_labels = np.squeeze(np.load(CLASSIFICATION_PATH + '/tmp/val_labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "bptt, embedding_size, num_hidden, num_layers = 70, 400, 1150, 3\n",
    "num_tokens = len(int_to_token)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([list([3, 4, 5, 2, 0, 906, 9, 17, 107, 235, 562, 8, 82, 27, 1568, 11, 551, 50, 2180, 3293]),\n",
       "        list([3, 4, 5, 2, 2239, 15, 143, 6, 15, 66, 15, 10, 9, 15, 10, 15, 48, 15, 299, 8, 8, 8, 289, 276]),\n",
       "        list([3, 4, 5, 2, 3834, 145, 6285, 33, 0, 152, 124, 3382]),\n",
       "        list([3, 4, 5, 2, 0, 0, 0, 100, 11, 5132, 7, 1080, 31, 51, 0, 25, 0, 7, 151, 54, 129, 2166, 130, 23, 11, 15, 0, 7]),\n",
       "        list([3, 4, 5, 2, 0, 6, 39, 71, 457, 12, 198, 58, 261, 20, 61, 6, 73])], dtype=object),\n",
       " array([0, 1, 0, 1, 1]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classification[:5], train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_label = train_labels.min()\n",
    "train_labels -= min_label\n",
    "val_labels -= min_label\n",
    "c = int(train_labels.max()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextDataset(train_classification, train_labels)\n",
    "val_ds = TextDataset(val_classification, val_labels)\n",
    "\n",
    "# the sortish sampler helps by sorting things kinda sorta by their token length so padding isn't crazy\n",
    "train_sampler = SortishSampler(train_classification, key=lambda x: len(train_classification[x]), bs=bs//2)\n",
    "# doesn't matter so much for the validation set\n",
    "val_sampler = SortSampler(val_classification, key=lambda x: len(val_classification[x]))\n",
    "\n",
    "# get data loaders\n",
    "train_dl = DataLoader(train_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=train_sampler)\n",
    "val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_sampler)\n",
    "\n",
    "# model data\n",
    "md = ModelData(PATH, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 1\n",
    "dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 2\n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_rnn_classifier(bptt, 20*70, c, num_tokens, emb_sz=embedding_size, n_hid=num_hidden, n_layers=num_layers,\n",
    "                       pad_token=1, layers=[embedding_size*3, 50, c], drops=[dps[4], 0.1], dropouti=dps[0],\n",
    "                       wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learn.clip=25.\n",
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=3e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr]) # differential learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load our encoder from our tweet language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 1e-7\n",
    "wd = 0\n",
    "learn.load_encoder('lm1_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all except last layer\n",
    "learn.freeze_to(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find learning rate\n",
    "learn.lr_find(lrs/1000)\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little tough to tell here, but we'll go with what we set previously and what fastai used for their imdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our first classifier\n",
    "learn.save('clas_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it back in\n",
    "learn.load('clas_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze one more layer\n",
    "learn.freeze_to(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our second classifier\n",
    "learn.save('clas_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it back in\n",
    "learn.load('clas_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze all layers so we're training the whole network\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out our loss\n",
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our final classifier\n",
    "learn.save('clas_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celebrity tweet sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('clas_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our celebrity tweets\n",
    "celebrity_df = pd.read_csv('/home/ubuntu/data/twitter/celebrity_tweets.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebrity_tokens, _ = get_texts(celebrity_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15833"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_token = pickle.load(open(LANGUAGE_MODEL_PATH + '/tmp/itos.pkl', 'rb'))\n",
    "token_to_int = collections.defaultdict(lambda: 0, {v:k for k, v in enumerate(int_to_token)})\n",
    "len(int_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebrity_classification = np.array([[token_to_int[o] for o in p] for p in celebrity_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebrity_ds = TextDataset(celebrity_classification, np.zeros(len(celebrity_classification), dtype=int))\n",
    "\n",
    "celebrity_dl = DataLoader(celebrity_ds, bs, transpose=True, num_workers=1, pad_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds = learn.predict_dl(celebrity_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3256, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(log_preds, axis=1)\n",
    "probs = np.exp(log_preds[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebrity_df = celebrity_df.assign(sentiment=pd.Series(preds))\n",
    "celebrity_df.to_csv('/home/ubuntu/data/twitter/celebrity_tweets_results.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "celebrity_to_tweets = {}\n",
    "for index, row in celebrity_df.iterrows():\n",
    "    if row[0] not in celebrity_to_tweets:\n",
    "        celebrity_to_tweets[row[0]] = []\n",
    "    else:\n",
    "        celebrity_to_tweets[row[0]].append({\n",
    "            'tweet': row[1],\n",
    "            'sentiment': row['sentiment']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BarackObama 0.7585227272727273\n",
      "realDonaldTrump 0.7182320441988951\n",
      "KimKardashian 0.852112676056338\n",
      "BillGates 0.772020725388601\n",
      "Oprah 0.8575197889182058\n",
      "justinbieber 0.9552845528455285\n",
      "TheRock 0.9039039039039038\n",
      "elonmusk 0.8461538461538461\n",
      "JeffBezos 0.9567901234567902\n",
      "katyperry 0.8897435897435897\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for screen_name, tweets in celebrity_to_tweets.items():\n",
    "    # remove any floaters from the csv load in\n",
    "    # small bug in the csv allows an occasional stray row in the dataframe resulting in an empty list that needs to be thrown out\n",
    "    if tweets:\n",
    "        avg_sentiment = np.mean([t['sentiment'] for t in tweets])\n",
    "        print(screen_name, avg_sentiment)\n",
    "        results.append((screen_name, avg_sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most postive celebrities on twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JeffBezos', 0.9567901234567902),\n",
       " ('justinbieber', 0.9552845528455285),\n",
       " ('TheRock', 0.9039039039039038),\n",
       " ('katyperry', 0.8897435897435897),\n",
       " ('Oprah', 0.8575197889182058),\n",
       " ('KimKardashian', 0.852112676056338),\n",
       " ('elonmusk', 0.8461538461538461),\n",
       " ('BillGates', 0.772020725388601),\n",
       " ('BarackObama', 0.7585227272727273),\n",
       " ('realDonaldTrump', 0.7182320441988951)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from bokeh.io import show, output_file\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.palettes import Spectral10\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "output_file(\"celebrity_tweet_sentimate.html\")\n",
    "\n",
    "handles = ['@' + x[0] for x in results]\n",
    "counts = [x[1] for x in results]\n",
    "counts = [int(x) for x in np.asarray(counts) * 100]\n",
    "\n",
    "source = ColumnDataSource(data=dict(handles=handles, counts=counts, color=Spectral10))\n",
    "\n",
    "p = figure(x_range=handles, y_range=(50,100), plot_height=400, title=\"Who's the most postive public figure on Twitter?\",\n",
    "           toolbar_location=None, tools=\"\")\n",
    "\n",
    "p.vbar(x='handles', top='counts', width=0.8, color='color', source=source, )\n",
    "\n",
    "p.xaxis.major_label_orientation = -math.pi/5\n",
    "p.min_border_right = 50\n",
    "p.yaxis.axis_label = \"% of tweets that are positive\"\n",
    "p.xaxis.axis_label = \"Twitter handle\"\n",
    "\n",
    "\n",
    "p.xgrid.grid_line_color = None\n",
    "p.legend.orientation = \"horizontal\"\n",
    "p.legend.location = \"top_center\"\n",
    "\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
